{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Overview of Modern NLP and Transformer Models\n\nModern Natural Language Processing (NLP) has evolved rapidly through pre-trained language models and transformer architectures, enabling tasks such as:\n\n1. Question Answering (QA) ‚Äì automatically generating answers to user questions.\n2. Text Classification ‚Äì categorizing text into predefined labels (e.g., spam, sentiment, topic).\n3. Text Summarization ‚Äì generating concise summaries that retain key ideas.\n4. Text Generation ‚Äì producing coherent and human-like text based on context.\n5. Code Generation ‚Äì converting natural language descriptions into source code.\n6. Image Generation ‚Äì creating images from textual descriptions.\n7. Chatbots ‚Äì conversational agents that simulate human dialogue.\n\nThese applications are powered by large-scale transformer-based models such as BERT, GPT, Mistral, Gemini, and other LLMs (Large Language Models) that understand and generate natural language.\n\n### Transformer Architecture ‚Äî Overview\n\nA transformer model consists of two main components:\n\n1. **Encoder:**  \n   Converts input text into contextual vector representations.  \n    [View PyTorch Encoder Example](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)\n\n2. **Decoder:**  \n   Generates output sequences (like translations or responses).  \n    [View PyTorch Decoder Example](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Model)\n\n\n\n### üîπ Key Components in Each Layer\n\n1. **Self-Attention Mechanism**  \n   Enables every token to \"attend\" to every other token in the input sequence.  \n   This helps capture relationships between words regardless of their position.  \n\n2. **Feed-Forward Neural Network**  \n   Applies nonlinear transformations and captures hidden relationships.  \n\n3. **Positional Encoding**  \n   Adds order-awareness so the model knows which word comes first or last.  \n\n\n\n### üí¨ Why Self-Attention Matters\nThe **self-attention mechanism** allows the transformer to understand global context ‚Äî  \nfor example, connecting ‚ÄúParis‚Äù with ‚ÄúFrance‚Äù even if they appear far apart in a sentence.\n\n> Without self-attention, models would only capture nearby relationships (like in RNNs),  \n> but transformers can capture long-range dependencies efficiently.\n\n\n### Key Models: BERT vs GPT\nModel\tType\tDirection\tStrengths\n- BERT (Bidirectional Encoder Representations from Transformers)\tEncoder-only\tBidirectional (reads left ‚Üî right)\tUnderstanding text ‚Üí good for classification, NER, QA\n- GPT (Generative Pre-trained Transformer)\tDecoder-only\tUnidirectional (reads left ‚Üí right)\tGenerating text ‚Üí good for text generation, summarization, chatbots\n\nBERT captures context from both sides, making it ideal for understanding language.\nGPT focuses on predicting the next token, enabling fluent and context-aware text generation.\n\n### How LLMs Work\n\n- Input ‚Äì The user provides text (question, prompt, or paragraph).\n- Tokenization & Embeddings ‚Äì The model splits text into tokens and converts them into dense numeric vectors.\n- Output Generation ‚Äì The model processes embeddings and produces results like text, code, summaries, or answers.\n\nLLMs are ‚Äúlarge‚Äù because they are trained on massive datasets (books, websites, code, etc.) and have millions to billions of parameters, which capture linguistic patterns and meaning.\n\n\n## Challenge: Sentiment Analysis Using DistilBERT\n\nIn this challenge, we‚Äôll perform **sentiment analysis** using the pre-trained **DistilBERT** model from Hugging Face Transformers.\n\nWe'll:\n- Load a small customer feedback dataset.\n- Apply the `distilbert-base-uncased-finetuned-sst-2-english` model.\n- Predict sentiment (Positive / Negative) and confidence scores.\n- Visualize results using a WordCloud, pie chart, and bar plot.\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:17:24.765128Z","iopub.execute_input":"2025-11-10T14:17:24.765446Z","iopub.status.idle":"2025-11-10T14:17:26.87594Z","shell.execute_reply.started":"2025-11-10T14:17:24.765417Z","shell.execute_reply":"2025-11-10T14:17:26.874884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings, os\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n# ============================================================\n#  Simple Transformer Example: BERT vs GPT\n# ============================================================\n\n# Install libraries (only needed once)\n# !pip install transformers torch --quiet\n\nfrom transformers import pipeline\n\n# ------------------------------------------------------------\n# BERT - Text Understanding (Sentiment Analysis)\n# ------------------------------------------------------------\nbert_classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ntext = \"I really love learning NLP with transformers!\"\nbert_result = bert_classifier(text)\n\nprint(\" Input Text:\", text)\nprint(\"BERT Sentiment Result:\", bert_result)\nprint(\"--------------------------------------------------\")\n\n# ------------------------------------------------------------\n#  GPT - Text Generation\n# ------------------------------------------------------------\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load GPT-2 model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\nprompt = \"Artificial Intelligence will transform education by\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Generate continuation\noutputs = model.generate(**inputs, max_new_tokens=40)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"GPT Prompt:\", prompt)\nprint(\"GPT Generated Text:\")\nprint(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:26:35.700926Z","iopub.execute_input":"2025-11-10T14:26:35.701396Z","iopub.status.idle":"2025-11-10T14:26:38.578686Z","shell.execute_reply.started":"2025-11-10T14:26:35.70137Z","shell.execute_reply":"2025-11-10T14:26:38.577764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  NLP Series ‚Äî BERT vs GPT\nThis notebook demonstrates two key Transformer models:\n\n| Model | Architecture | Task | Library |\n|--------|---------------|------|----------|\n| DistilBERT | Encoder-only | Sentiment Analysis | Hugging Face Transformers |\n| GPT-2 | Decoder-only | Text Generation | Hugging Face Transformers |\n\nBuilt using Kaggle GPU  \nLibraries: `transformers`, `torch`  \nDemonstrates text understanding vs. text generation\n","metadata":{}}]}