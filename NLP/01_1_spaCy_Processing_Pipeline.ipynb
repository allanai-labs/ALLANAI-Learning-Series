{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6cc3c1-f36c-4381-93a1-31479a57fd3e",
   "metadata": {},
   "source": [
    "### spaCy Processing Pipeline Challenge\n",
    "\n",
    "ALLANAI Learning Series — NLP Module\n",
    "\n",
    "#### Objective\n",
    "\n",
    "In this challenge, we’ll build a complete Natural Language Processing (NLP) pipeline using spaCy.\n",
    "You will process both:\n",
    "\n",
    "- A single sentence, and\n",
    "- A sentiments dataset (CSV file)\n",
    "\n",
    "The pipeline will include tokenization, stopword removal, part-of-speech tagging, dependency parsing, lemmatization, and named entity recognition (NER).\n",
    "\n",
    "#### Step 1: Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1752ef4-cadb-4dcb-9207-c35afc580a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Download spaCy English model (run once)\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba52e2d-6b84-4487-ba24-c7f442bb325c",
   "metadata": {},
   "source": [
    "#### Step 2: Single Sentence Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c704cf4-1551-456d-9dc4-955bc883f9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokens and POS Tags ===\n",
      "London         PROPN     nsubj\n",
      "is             AUX       ROOT\n",
      "a              DET       det\n",
      "beautiful      ADJ       amod\n",
      "city           NOUN      attr\n",
      ".              PUNCT     punct\n",
      "Mahira         PROPN     nsubj\n",
      "is             AUX       aux\n",
      "learning       VERB      ROOT\n",
      "NLP            PROPN     dobj\n",
      "with           ADP       prep\n",
      "ALLANAI        PROPN     compound\n",
      "Labs           PROPN     pobj\n",
      ".              PUNCT     punct\n",
      "\n",
      "=== Named Entities ===\n",
      "London                    → GPE\n",
      "Mahira                    → PERSON\n",
      "NLP                       → ORG\n",
      "ALLANAI Labs              → ORG\n"
     ]
    }
   ],
   "source": [
    "text = \"London is a beautiful city. Mahira is learning NLP with ALLANAI Labs.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"=== Tokens and POS Tags ===\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15}{token.pos_:<10}{token.dep_}\")\n",
    "\n",
    "print(\"\\n=== Named Entities ===\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<25} → {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d824ea-7afc-4862-b433-cd329a5ae9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokens and POS Tags ===\n",
      "London         PROPN     nsubj\n",
      "is             AUX       ROOT\n",
      "a              DET       det\n",
      "beautiful      ADJ       amod\n",
      "city           NOUN      attr\n",
      ".              PUNCT     punct\n",
      "Mahira         PROPN     nsubj\n",
      "is             AUX       aux\n",
      "learning       VERB      ROOT\n",
      "NLP            PROPN     dobj\n",
      "with           ADP       prep\n",
      "ALLANAI        PROPN     compound\n",
      "Labs           PROPN     pobj\n",
      ".              PUNCT     punct\n"
     ]
    }
   ],
   "source": [
    "text = \"London is a beautiful city. Mahira is learning NLP with ALLANAI Labs.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"=== Tokens and POS Tags ===\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15}{token.pos_:<10}{token.dep_}\") # Prints each token, its POS tag, and dependency label in neat aligned columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688789e-c347-406e-a9e1-ff8bd48067f5",
   "metadata": {},
   "source": [
    "#### Step 3: Tokenization, Stopwords & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "261d4482-1bcf-4d88-b72b-edbcc2f5ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('London', 'London'), ('beautiful', 'beautiful'), ('city', 'city'), ('Mahira', 'Mahira'), ('learning', 'learn'), ('NLP', 'NLP'), ('ALLANAI', 'ALLANAI'), ('Labs', 'Labs')]\n"
     ]
    }
   ],
   "source": [
    "filtered = [(token.text,token.lemma_) for token in doc if not token.is_stop and not token.is_punct]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be453f5b-d96e-40ae-8edf-aaaad4493e4d",
   "metadata": {},
   "source": [
    "#### Step 4: Mock Sentiment Dataset\n",
    "\n",
    "Instead of reading a file, we’ll create our own small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a088c81-b577-4f7e-9938-d064238bd669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this product, it’s amazing!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The service was slow and disappointing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wonderful experience, highly recommend it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not worth the price at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The quality was okay but delivery was late.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text\n",
       "0           I love this product, it’s amazing!\n",
       "1      The service was slow and disappointing.\n",
       "2   Wonderful experience, highly recommend it.\n",
       "3                  Not worth the price at all.\n",
       "4  The quality was okay but delivery was late."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"text\": [\n",
    "        \"I love this product, it’s amazing!\",\n",
    "        \"The service was slow and disappointing.\",\n",
    "        \"Wonderful experience, highly recommend it.\",\n",
    "        \"Not worth the price at all.\",\n",
    "        \"The quality was okay but delivery was late.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c8b68-b9dc-4f60-bb3e-3e2f0e6f9d53",
   "metadata": {},
   "source": [
    "#### Step 5: Apply spaCy Pipeline to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ce8b13f-a10f-4057-8d35-572c1780c763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Cleaned_Tokens</th>\n",
       "      <th>Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this product, it’s amazing!</td>\n",
       "      <td>love product amazing</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The service was slow and disappointing.</td>\n",
       "      <td>service slow disappointing</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wonderful experience, highly recommend it.</td>\n",
       "      <td>wonderful experience highly recommend</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not worth the price at all.</td>\n",
       "      <td>worth price</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The quality was okay but delivery was late.</td>\n",
       "      <td>quality okay delivery late</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Sentence  \\\n",
       "0           I love this product, it’s amazing!   \n",
       "1      The service was slow and disappointing.   \n",
       "2   Wonderful experience, highly recommend it.   \n",
       "3                  Not worth the price at all.   \n",
       "4  The quality was okay but delivery was late.   \n",
       "\n",
       "                          Cleaned_Tokens Entities  \n",
       "0                   love product amazing       []  \n",
       "1             service slow disappointing       []  \n",
       "2  wonderful experience highly recommend       []  \n",
       "3                            worth price       []  \n",
       "4             quality okay delivery late       []  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for line in df['text']:\n",
    "    doc = nlp(line)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    results.append({\n",
    "        \"Sentence\": line,\n",
    "        \"Cleaned_Tokens\": \" \".join(lemmas),\n",
    "        \"Entities\": entities\n",
    "    })\n",
    "\n",
    "processed_df = pd.DataFrame(results)\n",
    "processed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9334d-e203-440f-becb-3d71b8402024",
   "metadata": {},
   "source": [
    "#### Step 6: Summary\n",
    "\n",
    "You’ve now built a complete spaCy NLP pipeline that can handle raw text or small datasets — all without needing any external files.\n",
    "\n",
    "This forms the foundation for sentiment analysis, keyword extraction, or text classification in future notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
